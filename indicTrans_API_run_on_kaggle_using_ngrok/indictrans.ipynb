{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyngrok\n!pip install wurlitzer\n!pip uninstall keras -y\n!pip install keras==2.15.0\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T04:59:42.610507Z","iopub.execute_input":"2024-04-13T04:59:42.610909Z","iopub.status.idle":"2024-04-13T05:00:27.660550Z","shell.execute_reply.started":"2024-04-13T04:59:42.610874Z","shell.execute_reply":"2024-04-13T05:00:27.659362Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyngrok\n  Downloading pyngrok-7.1.6-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.10/site-packages (from pyngrok) (6.0.1)\nDownloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\nInstalling collected packages: pyngrok\nSuccessfully installed pyngrok-7.1.6\nCollecting wurlitzer\n  Downloading wurlitzer-3.0.3-py3-none-any.whl.metadata (1.9 kB)\nDownloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\nInstalling collected packages: wurlitzer\nSuccessfully installed wurlitzer-3.0.3\nFound existing installation: keras 3.1.1\nUninstalling keras-3.1.1:\n  Successfully uninstalled keras-3.1.1\nCollecting keras==2.15.0\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: keras\nSuccessfully installed keras-2.15.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Clone the github repository and navigate to the project directory.\n!git clone https://github.com/AI4Bharat/IndicTrans2\n%cd /kaggle/working/IndicTrans2/huggingface_interface\n!python3 -m pip install nltk sacremoses pandas regex mock transformers>=4.33.2 mosestokenizer\n!python3 -c \"import nltk; nltk.download('punkt')\"\n!python3 -m pip install bitsandbytes scipy accelerate datasets\n!python3 -m pip install sentencepiece\n!git clone https://github.com/VarunGumma/IndicTransTokenizer\n%cd IndicTransTokenizer\n!python3 -m pip install --editable ./","metadata":{"execution":{"iopub.status.busy":"2024-04-13T05:00:27.662704Z","iopub.execute_input":"2024-04-13T05:00:27.663040Z","iopub.status.idle":"2024-04-13T05:02:07.866594Z","shell.execute_reply.started":"2024-04-13T05:00:27.663010Z","shell.execute_reply":"2024-04-13T05:02:07.865403Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'IndicTrans2'...\nremote: Enumerating objects: 668, done.\u001b[K\nremote: Counting objects: 100% (151/151), done.\u001b[K\nremote: Compressing objects: 100% (81/81), done.\u001b[K\nremote: Total 668 (delta 108), reused 77 (delta 70), pack-reused 517\u001b[K\nReceiving objects: 100% (668/668), 4.12 MiB | 18.92 MiB/s, done.\nResolving deltas: 100% (415/415), done.\n/kaggle/working/IndicTrans2/huggingface_interface\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.28.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.1\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nCloning into 'IndicTransTokenizer'...\nremote: Enumerating objects: 123, done.\u001b[K\nremote: Counting objects: 100% (123/123), done.\u001b[K\nremote: Compressing objects: 100% (80/80), done.\u001b[K\nremote: Total 123 (delta 52), reused 94 (delta 31), pack-reused 0\u001b[K\nReceiving objects: 100% (123/123), 3.85 MiB | 20.03 MiB/s, done.\nResolving deltas: 100% (52/52), done.\n/kaggle/working/IndicTrans2/huggingface_interface/IndicTransTokenizer\nObtaining file:///kaggle/working/IndicTrans2/huggingface_interface/IndicTransTokenizer\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library (from IndicTransTokenizer==0.1.3)\n  Cloning https://github.com/VarunGumma/indic_nlp_library to /tmp/pip-install-wxtxkkta/indic-nlp-library-it2_670f0b05593d416c86b9c8d23374e3f1\n  Running command git clone --filter=blob:none --quiet https://github.com/VarunGumma/indic_nlp_library /tmp/pip-install-wxtxkkta/indic-nlp-library-it2_670f0b05593d416c86b9c8d23374e3f1\n  Resolved https://github.com/VarunGumma/indic_nlp_library to commit 1dd6683a6dd77be3c1dbe03c226201661235c72b\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting setuptools==68.2.2 (from IndicTransTokenizer==0.1.3)\n  Downloading setuptools-68.2.2-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from IndicTransTokenizer==0.1.3) (2.1.2)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.10/site-packages (from IndicTransTokenizer==0.1.3) (0.1.1)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from IndicTransTokenizer==0.1.3) (0.2.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from IndicTransTokenizer==0.1.3) (4.39.3)\nCollecting sacrebleu==2.3.1 (from IndicTransTokenizer==0.1.3)\n  Downloading sacrebleu-2.3.1-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu==2.3.1->IndicTransTokenizer==0.1.3)\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu==2.3.1->IndicTransTokenizer==0.1.3) (2023.12.25)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu==2.3.1->IndicTransTokenizer==0.1.3) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu==2.3.1->IndicTransTokenizer==0.1.3) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu==2.3.1->IndicTransTokenizer==0.1.3) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu==2.3.1->IndicTransTokenizer==0.1.3) (5.2.1)\nCollecting sphinx-argparse (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3)\n  Downloading sphinx_argparse-0.4.0-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: sphinx_rtd_theme in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (0.2.4)\nCollecting morfessor (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3)\n  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (2.1.4)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sacremoses->IndicTransTokenizer==0.1.3) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses->IndicTransTokenizer==0.1.3) (1.3.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sacremoses->IndicTransTokenizer==0.1.3) (4.66.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->IndicTransTokenizer==0.1.3) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->IndicTransTokenizer==0.1.3) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->IndicTransTokenizer==0.1.3) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->IndicTransTokenizer==0.1.3) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->IndicTransTokenizer==0.1.3) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->IndicTransTokenizer==0.1.3) (2024.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers->IndicTransTokenizer==0.1.3) (0.22.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->IndicTransTokenizer==0.1.3) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->IndicTransTokenizer==0.1.3) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->IndicTransTokenizer==0.1.3) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->IndicTransTokenizer==0.1.3) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers->IndicTransTokenizer==0.1.3) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers->IndicTransTokenizer==0.1.3) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->IndicTransTokenizer==0.1.3) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->IndicTransTokenizer==0.1.3) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->IndicTransTokenizer==0.1.3) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->IndicTransTokenizer==0.1.3) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->IndicTransTokenizer==0.1.3) (2024.2.2)\nCollecting sphinx>=1.2.0 (from sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3)\n  Downloading sphinx-7.2.6-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->IndicTransTokenizer==0.1.3) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (1.16.0)\nCollecting sphinxcontrib-applehelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3)\n  Downloading sphinxcontrib_applehelp-1.0.8-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-devhelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3)\n  Downloading sphinxcontrib_devhelp-1.0.6-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-jsmath (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3)\n  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting sphinxcontrib-htmlhelp>=2.0.0 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3)\n  Downloading sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3)\n  Downloading sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl.metadata (2.4 kB)\nCollecting sphinxcontrib-qthelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3)\n  Downloading sphinxcontrib_qthelp-1.0.7-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: Pygments>=2.14 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (2.17.2)\nRequirement already satisfied: docutils<0.21,>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (0.20.1)\nRequirement already satisfied: snowballstemmer>=2.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (2.2.0)\nRequirement already satisfied: babel>=2.9 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (2.14.0)\nCollecting alabaster<0.8,>=0.7 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3)\n  Downloading alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\nCollecting imagesize>=1.3 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3)\n  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\nDownloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading setuptools-68.2.2-py3-none-any.whl (807 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\nDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nDownloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\nDownloading sphinx-7.2.6-py3-none-any.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading alabaster-0.7.16-py3-none-any.whl (13 kB)\nDownloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\nDownloading sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl (92 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_applehelp-1.0.8-py3-none-any.whl (120 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_devhelp-1.0.6-py3-none-any.whl (83 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\nDownloading sphinxcontrib_qthelp-1.0.7-py3-none-any.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: indic-nlp-library-IT2\n  Building wheel for indic-nlp-library-IT2 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for indic-nlp-library-IT2: filename=indic_nlp_library_IT2-0.0.1-py3-none-any.whl size=56123 sha256=3e8fbf176d23bb2a51617ac3f688169ff325c1e2ecdefd05a46498b33f07986d\n  Stored in directory: /tmp/pip-ephem-wheel-cache-1nggi9v6/wheels/e9/72/fa/bd9f19a3f2bacb50efcaf28b7ab89fe7ca539e35b75334befc\nSuccessfully built indic-nlp-library-IT2\nInstalling collected packages: morfessor, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, setuptools, portalocker, imagesize, alabaster, sphinx, sacrebleu, sphinx-argparse, indic-nlp-library-IT2, IndicTransTokenizer\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 69.0.3\n    Uninstalling setuptools-69.0.3:\n      Successfully uninstalled setuptools-69.0.3\n  Running setup.py develop for IndicTransTokenizer\nSuccessfully installed IndicTransTokenizer-0.1.3 alabaster-0.7.16 imagesize-1.4.1 indic-nlp-library-IT2-0.0.1 morfessor-2.0.6 portalocker-2.8.2 sacrebleu-2.3.1 setuptools-68.2.2 sphinx-7.2.6 sphinx-argparse-0.4.0 sphinxcontrib-applehelp-1.0.8 sphinxcontrib-devhelp-1.0.6 sphinxcontrib-htmlhelp-2.0.5 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.7 sphinxcontrib-serializinghtml-1.1.10\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig\nfrom IndicTransTokenizer import IndicProcessor, IndicTransTokenizer\n\nBATCH_SIZE = 4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquantization = None","metadata":{"execution":{"iopub.status.busy":"2024-04-13T05:02:07.868121Z","iopub.execute_input":"2024-04-13T05:02:07.868445Z","iopub.status.idle":"2024-04-13T05:02:15.147850Z","shell.execute_reply.started":"2024-04-13T05:02:07.868417Z","shell.execute_reply":"2024-04-13T05:02:15.146901Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def initialize_model_and_tokenizer(ckpt_dir, direction, quantization):\n    if quantization == \"4-bit\":\n        qconfig = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_compute_dtype=torch.bfloat16,\n        )\n    elif quantization == \"8-bit\":\n        qconfig = BitsAndBytesConfig(\n            load_in_8bit=True,\n            bnb_8bit_use_double_quant=True,\n            bnb_8bit_compute_dtype=torch.bfloat16,\n        )\n    else:\n        qconfig = None\n\n    tokenizer = IndicTransTokenizer(direction=direction)\n    model = AutoModelForSeq2SeqLM.from_pretrained(\n        ckpt_dir,\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n        quantization_config=qconfig,\n    )\n\n    if qconfig == None:\n        model = model.to(DEVICE)\n        if DEVICE == \"cuda\":\n            model.half()\n\n    model.eval()\n\n    return tokenizer, model\n\n\ndef batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n    translations = []\n    for i in range(0, len(input_sentences), BATCH_SIZE):\n        batch = input_sentences[i : i + BATCH_SIZE]\n\n        # Preprocess the batch and extract entity mappings\n        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n\n        # Tokenize the batch and generate input encodings\n        inputs = tokenizer(\n            batch,\n            src=True,\n            truncation=True,\n            padding=\"longest\",\n            return_tensors=\"pt\",\n            return_attention_mask=True,\n        ).to(DEVICE)\n\n        # Generate translations using the model\n        with torch.no_grad():\n            generated_tokens = model.generate(\n                **inputs,\n                use_cache=True,\n                min_length=0,\n                max_length=256,\n                num_beams=5,\n                num_return_sequences=1,\n            )\n\n        # Decode the generated tokens into text\n        generated_tokens = tokenizer.batch_decode(generated_tokens.detach().cpu().tolist(), src=False)\n\n        # Postprocess the translations, including entity replacement\n        translations += ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n\n        del inputs\n        torch.cuda.empty_cache()\n\n    return translations","metadata":{"execution":{"iopub.status.busy":"2024-04-13T05:02:15.150371Z","iopub.execute_input":"2024-04-13T05:02:15.150817Z","iopub.status.idle":"2024-04-13T05:02:15.163212Z","shell.execute_reply.started":"2024-04-13T05:02:15.150790Z","shell.execute_reply":"2024-04-13T05:02:15.162053Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-dist-200M\"  # ai4bharat/indictrans2-en-indic-dist-200M\nen_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, \"en-indic\", quantization)\n\nip = IndicProcessor(inference=True)\n\nen_sents = [\n    \"When I was young, I used to go to the park every day.\",\n    \"He has many old books, which he inherited from his ancestors.\",\n]\n\nsrc_lang, tgt_lang = \"eng_Latn\", \"tam_Taml\"\nhi_translations = batch_translate(en_sents, src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip)\n\nprint(f\"\\n{src_lang} - {tgt_lang}\")\nfor input_sentence, translation in zip(en_sents, hi_translations):\n    print(f\"{src_lang}: {input_sentence}\")\n    print(f\"{tgt_lang}: {translation}\")\n\ndel en_indic_tokenizer, en_indic_model","metadata":{"execution":{"iopub.status.busy":"2024-04-13T05:02:15.164596Z","iopub.execute_input":"2024-04-13T05:02:15.164904Z","iopub.status.idle":"2024-04-13T05:02:44.214631Z","shell.execute_reply.started":"2024-04-13T05:02:15.164879Z","shell.execute_reply":"2024-04-13T05:02:44.213516Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3201a3d57b4470b92cc367a62849409"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_indictrans.py:   0%|          | 0.00/14.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66b97cde933342409189655319af3f5d"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-dist-200M:\n- configuration_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_indictrans.py:   0%|          | 0.00/61.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4be12ae2a64841b3b90abd4a8cd109a2"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-dist-200M:\n- modeling_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6afb61c167945e4a6f72df2026ec0c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80dad6237cd345c19ef35fd4fb302aa7"}},"metadata":{}},{"name":"stdout","text":"\neng_Latn - tam_Taml\neng_Latn: When I was young, I used to go to the park every day.\ntam_Taml: நான் இளமையாக இருந்தபோது, ஒவ்வொரு நாளும் பூங்காவுக்குச் செல்வது வழக்கம்.\neng_Latn: He has many old books, which he inherited from his ancestors.\ntam_Taml: அவர் தனது மூதாதையர்களிடமிருந்து மரபுரிமையாகப் பெற்ற பல பழைய புத்தகங்கள் அவரிடம் உள்ளன.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize the model and tokenizer\nen_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-dist-200M\"  # Change to your actual checkpoint directory\nen_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, \"en-indic\", quantization)  # Change the quantization if needed\nip = IndicProcessor(inference=True)  # Initialize your processor here","metadata":{"execution":{"iopub.status.busy":"2024-04-13T05:02:44.216429Z","iopub.execute_input":"2024-04-13T05:02:44.216965Z","iopub.status.idle":"2024-04-13T05:02:45.498933Z","shell.execute_reply.started":"2024-04-13T05:02:44.216934Z","shell.execute_reply":"2024-04-13T05:02:45.498090Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from flask import Flask, jsonify, request\nfrom pyngrok import ngrok\n\napp = Flask(__name__)\nngrok.set_auth_token(\"2eOCN9e1AgJi9oIB6MfQCfKZUpd_5rXsgn1oTq6gKoMXQ2j2g\")\n\n@app.route(\"/translate\", methods=[\"POST\"])\ndef translate():\n    \n    data = request.json\n    title = data.get('title')\n    body = data.get('body')\n    target_lang = data.get('target_lang')\n    \n    src_lang = \"eng_Latn\"\n    \n    \n    if not title or not body:\n        return jsonify({\"error\": \"Request must contain title and body fields.\"}), 400\n    \n    \n    input_sentences = [title, body]\n\n    # Translate\n    translations = batch_translate(input_sentences, src_lang, target_lang, en_indic_model, en_indic_tokenizer, ip)\n\n    \n    translated_data = {\n        'title': translations[0],\n        'body': translations[1]\n    }\n    return jsonify(translated_data), 200\n\n# ngrok tunnel\nif __name__ == \"__main__\":\n    public_url = ngrok.connect(5000)\n    print(\" * ngrok tunnel \\\"{}\\\" has been opened\".format(public_url))\n    \n    # Run Flask app\n    app.run(port=5000)\n    \n    \n    ngrok.disconnect(public_url)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T05:02:45.499964Z","iopub.execute_input":"2024-04-13T05:02:45.500265Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":" * ngrok tunnel \"NgrokTunnel: \"https://7ffe-34-73-98-209.ngrok-free.app\" -> \"http://localhost:5000\"\" has been opened\n * Serving Flask app '__main__'\n * Debug mode: off\n","output_type":"stream"}]},{"cell_type":"code","source":"# from flask import Flask, jsonify, request\n# from pyngrok import ngrok\n\n# app = Flask(__name__)\n\n\n# port = 5000\n# ngrok.set_auth_token(\"2eOCN9e1AgJi9oIB6MfQCfKZUpd_5rXsgn1oTq6gKoMXQ2j2g\")\n# @app.route(\"/\")\n# def hello():\n#     return \"Hello World!! from Google Colab\"\n\n# # Start ngrok when app is run\n# if __name__ == \"__main__\":\n#     # Setup ngrok\n#     public_url = ngrok.connect(port)\n#     print(\" * ngrok tunnel \\\"{}\\\" has been opened\".format(public_url))\n    \n#     # Provide the Flask app to run on the specified port\n#     app.run(port=port)\n    \n#     ngrok.disconnect(public_url)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}